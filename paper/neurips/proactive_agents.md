\section{Formulation}

Suppose we have trial $\mathcal{T}$  drawn from a distribution of trials $\mu(\mathcal{T})$.   Each trial $\mathcal{T}$ consists of some initial contextual information $\mathcal{I}$ (e.g. description of a game, number of episodes to play, associated evaluation which will be considered) and a task $\mathcal{M}$ associated with it. Some examples of task description are as follows:

\begin{enumerate}
    \item \textbf{Single episode:} Play task $\mathcal{M}$ once and try to collect maximum reward. Description of the game $\mathcal{M} ...$  \comment{For now we can focus on this}
    \item \textbf{Multiple episodes with exploratory phase:} Play task $\mathcal{M}$,  $K-1$ times so that you collect maximum reward in the $K^{th}$ play. Description of the task $\mathcal{M} ...$  
    \item \textbf{Consistently improve over episodes:} Play task $\mathcal{M}$, $K$ times and consistently improve over it. Description of the task $\mathcal{M} ...$  
\end{enumerate}

The task $\mathcal{M}$ starts in some state $X_0 \in \mathcal{S}$, we take an action $A_0 \in \mathcal{A}$, it transitions to state $X_1$, and so on. Let $S_{t+1} \sim P_S(\cdot|S_t,A_t)$ denote the transition functions that determines the dynamics of this game and has an associated . 
Further we assume that the reward $R$ depends on the whole trajectory of $R_t \sim P_R(\cdot|S_t,A_t, S_{t+1})$. Therefore game $\mathcal{M} \equiv \{\mathcal{S}, \mathcal{A}, P_S, P_{R}\}$. Further, let horizon be $T$ and total reward collected is $\sum_{t=0}^{T-1} R_t$.
In general we can assume that transition function, and reward function depends on the whole history of the game including just terminal rewards. 


Now the trial is $\mathcal{T} \equiv \{\mathcal{I}, \mathcal{M}\}$. Consider a policy $\pi = (\pi_{0},\pi_{1},\cdots ) \in \Pi$, such that $a_k = \pi_k(\mathcal{H}_k) \in \mathcal{A}$ determines the action depending on  all the history $\mathcal{H}_k = (X_0,A_0,R_0\cdots,X_{k-1},A_{k-1},R_{k-1},X_k) $. We will call it $\mathcal{H}_k$. Let $\tau$ be the complete trajectory generated by playing policy $\pi$ on task $\mathcal{T}$. 


\[\max_{\pi} \left[ \E_{\mathcal{T}\sim \mu(\cdot)} \left[ \E_{\tau \sim p(\cdot|\mathcal{T}, \pi)} \left( R(\tau) \right)\right]\right]\]

where $R(\tau)$ is reward determined based on task description and the corresponding game.

\vspace{3mm}
\noindent\textbf{Solving each task $\mathcal{T}$ is an RL problem}  - we will call it \textbf{inner-loop RL} and the corresponding RL policy is $\pi$. We will parametrize our policy $\pi$ using LLM ($\pi_\phi$). 

\vspace{3mm}
\noindent \textbf{How to optimize $\pi_\phi$?} - Optimizing $\pi_\phi$ is itself an RL problem and we will call it as \textbf{outer-loop RL}. 



\subsection{Optimizing $\pi_\phi$}

Lets forget about \textbf{inner-loop RL} and just think about \textbf{outer-loop RL}. We can use something like the following in Algorithm \ref{alg:RL2}. We can use similar various online RL algorithms to optimize for $\pi_\phi$: GRPO,  PPO,  DPO.



\begin{algorithm}
\caption{Optimizing $\pi_\phi$ using policy gradients on $\phi$}
\begin{algorithmic}[1]
\STATE Initialize parameters $\phi$ (LLM)
\WHILE{not done}
    \STATE Sample trials $\mathcal{T}^i \sim \mu(\mathcal{T})$
    \FOR{each trial $\mathcal{T}^i$}
        \STATE Current LLM policy is $\phi$
        \STATE Run each trial consisting of all the episodes using current LLM policy $\phi_0$
        \FOR{each timestep $t$ in the trial}
            \STATE Sample action $A_t \sim \pi_{\phi}(\cdot|\mathcal{H}_t)$
            \STATE Execute action $A_t$ and receive reward $R_t$
        \ENDFOR
    \ENDFOR
    \STATE Update parameter $\phi$ by optimizing the expected discounted sum of rewards across the trial (taking gradients).
\ENDWHILE
\end{algorithmic}
\label{alg:RL2}
\end{algorithm}










\section{Training Choices for $\pi_\phi$ and parametrizations}





\subsection{Training choices} 

\begin{itemize}
\item Dense rewards, terminal rewards: We can study the scaling behavior of both approaches and choose the one which scales better. 
\item GRPO, PPO, DPO: Similarly here we can either fix one strategy or actually see the scaling behavior and choose based on that.
\end{itemize}









\subsection{Different choices of the $\pi_\phi$ possible}

We can consider different LLM policies - let $\phi$ be the LLM parameters and $G(LLM_\phi)$ be the overall policy according to which we sample next action, that is, $A_t = G(LLM_\phi(H_t))$. For example, $G$ might represent doing $N$ roll-outs and picking an action that is best of $N$.  

\begin{enumerate}
    \item \textbf{$G\equiv$ No-rollout} that is $ A_t = G(LLM_\phi(H_t)) =  LLM_\phi(H_t)$ - \textcolor{red}{\textbf{We will focus on this for now}}
    \item \textbf{$G \equiv$ $N$ parallel roll-outs and best of $N$ variants (No optimization of $G$)}. Doing roll-out requires ability to predict $R_t$ and $X_{t+1}$ based on $H_t$ and $A_t=G(LLM_\phi(H_t))$.  Therefore we have to train our LLM to predict future trajectories as well  - \textbf{imagined roll-outs}.

    \textbf{Note:} No optimization of roll-outs (no optimization of $G$)  during training (improving inference-scaling).

    \item \textbf{$G \equiv$ $N$ parallel roll-outs and best of $N$ varaints (Optimization of $G$)} . Doing roll-out requires ability to predict $R_t$ and $X_{t+1}$ based on $H_t$ and $A_t=G(LLM_\phi(H_t))$.  Therefore we have to train our LLM to predict future trajectories as well.

    \textbf{Note:} We will optimize roll-outs (optimization of $G$)  during training (improving inference-scaling).
    
    \item  \textbf{$G \equiv$ $N$ sequential roll-outs (No optimization of $G$)} . Doing roll-out requires ability to predict $R_t$ and $X_{t+1}$ based on $H_t$ and $A_t=G(LLM_\phi(H_t))$.  Therefore we have to train our LLM to predict future trajectories as well.

    It will look like $[Rollout-1, <wait>, <think-again>, Rollout-2, \cdots]$

    \textbf{Note:} No optimization of roll-outs (no optimization of $G$)  during training (improving inference-scaling).
    
    \item  \textbf{$G \equiv$ $N$ sequential roll-outs (Optimization of $G$)} . Doing roll-out requires ability to predict $R_t$ and $X_{t+1}$ based on $H_t$ and $A_t=G(LLM_\phi(H_t))$.  Therefore we have to train our LLM to predict future trajectories as well.

    It will look like $[Rollout-1, <wait>, <think-again>, Rollout-2, \cdots]$

    \textbf{Note:} Optimization of roll-outs (Optimization of $G$)  during training (improving inference-scaling)  - it might require defining intermediate rewards. 
\end{enumerate}




\section{Tasks we want to consider}

\begin{itemize}
    \item Text games
    \item Teacher-Student
    \item Zico's paper?
\end{itemize}

How to generate these tasks - broader discussion

\subsection{Testing/Evaluation}

\begin{itemize}
    \item Fixing a sample of tasks: in-distribution, out-of-distribution
\item Fix few datasets: MMLU etc.
\item How should we evaluate reasoning-transfer? Accuracy might confound knowledge transfers. If we measure  of interaction, it might better capture transfer of “reasoning”
\item Metrics, visualization
\end{itemize}













\newpage
\section{Principled way of thinking about this}




Consider an Markov Decision Process with unknown transition function, defined by following dynamics - 

\begin{align}
   X_{k+1} = f_\theta(X_k,A_k,\epsilon_k) \quad \text{for} \quad k\in\{0,1,\cdots\} 
   \label{eq:dynamics}
\end{align}


where
 $X_k \in \mathcal{X}$ represents the ``state'' of the system,  $A_k \in \mathcal{A}$ is ``action'', $\epsilon_k$ is the random disturbance with $(\epsilon_k)_{k\geq 0}$ assumed to be i.i.d. Transition function $f$ is parametrized by $\theta$  introduces a probability transition function  $P_{\theta} (X_{k+1} |X_k, A_k)$. Further, $\theta \sim \mu$ presents uncertainty over the transition function. At time step $k$ we receive reward $R_k = g(X_k,A_k,w_k)$ where $w_k$ is the random disturbance with $(w_k)_{k\geq 0 }$ i.i.d. and independent of $(\epsilon_k)_{k\geq 0 }$. 



% Note that, we have transition function and reward function both parametrized by $\theta$ and have noise $\epsilon_k$. Therefore, it introduces a probability transition function  $P_{\theta} (X_{k+1}, R_k |X_k, A_k)$, parametrized by $\theta$. Here $\theta \sim \mu$ presents uncertainty over the transition function and reward function. In other words, uncertainty over the task we are trying to solve.

\vspace{2mm}
\noindent \textbf{Policy:} Consider a policy $\pi = (\pi_{0},\pi_{1},\cdots , \pi_{T-1}) \in \Pi$, such that $a_k = \pi_k(\mathcal{H}_k) \in \mathcal{A}$ determines the action depending on  all the history $\mathcal{H}_k = (X_0,A_0,\cdots,X_{k-1},A_{k-1},X_k) $. 

\vspace{2mm}
\noindent \textbf{Objective:}  We want to maximize the following objective

\begin{align}
 \max_{\pi\in\Pi} \E^\pi \left[ \sum_{k=0}^{T-1} R_k \right] =  \max_{\pi\in\Pi} \E^\pi \left[ \sum_{k=0}^{T-1} g_{\theta}(X_k,A_k,\epsilon_k)\right]
 \label{eq:objective}
\end{align}
where expectation is with respect to the randomness of $\theta$, policy $\pi$, noise $\epsilon_k$'s and $w_k$'s.

\vspace{2mm}
\noindent \textbf{Note:} For now, we assume that reward function is known, we can easily extend it to the unknown reward function case - which is essential for the scenarios we are dealing with. 
